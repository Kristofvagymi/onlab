{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web sraping tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from urllib3.exceptions import InsecureRequestWarning\n",
    "requests.packages.urllib3.disable_warnings(category=InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare 'global' variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# desktop user-agent\n",
    "USER_AGENT = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.129 Safari/537.36\"\n",
    "# mobile user-agent\n",
    "MOBILE_USER_AGENT = \"Mozilla/5.0 (Linux; Android 7.0; SM-G930V Build/NRD90M) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.125 Mobile Safari/537.36\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"user-agent\" : USER_AGENT}\n",
    "categories = ['food', 'disease', 'chemistry']\n",
    "keyword_groups = [['peach', 'carrot', 'tomato'], ['coronavirus', 'cancer', 'food poisoning'], ['ch4', 'co2', 'n2o']]\n",
    "path_to_save = 'C:\\\\Users\\\\ivani\\\\source\\\\python\\\\msc_onlab1\\\\pubmed\\\\data'\n",
    "base_url_path = 'https://pubmed.ncbi.nlm.nih.gov'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method gives back {n} number of webpage {url and title} dictinaries<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_firt_n_url(disp, category, n): \n",
    "    headers = {\"user-agent\" : USER_AGENT}\n",
    "    results = []\n",
    "    i = 0\n",
    "    \n",
    "    while len(list({r['link']:r for r in results}.values())) < n:\n",
    "        \n",
    "        query_url = f\"{base_url_path}/?term={category}&page=\" +str(i)\n",
    "        response = requests.get(query_url, headers=headers)\n",
    "        i += 1\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        for g in soup.find_all('a', class_='labs-docsum-title'):\n",
    "            item = {\n",
    "                    \"title\": g.get_text().replace('\\n', ''),\n",
    "                    \"link\":g['href']\n",
    "                }\n",
    "            results.append(item)\n",
    "        disp.update(category+'~'+str( min( round(len(list({r['link']:r for r in results}.values()))/n, 2), 1.0) *100)+\"%\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iter on keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Process finished'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url_groups = []\n",
    "dh = display('Process started',display_id=True)\n",
    "for category, keyword_group in zip(categories, keyword_groups):\n",
    "    url_group = []\n",
    "    for keyword in keyword_group:\n",
    "        url_group += get_firt_n_url(dh, keyword, 100)\n",
    "    url_groups.append(url_group)\n",
    "dh.update('Process finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Create dir for html files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created the directory C:\\Users\\ivani\\source\\python\\msc_onlab1\\pubmed\\data\\food \n",
      "Successfully created the directory C:\\Users\\ivani\\source\\python\\msc_onlab1\\pubmed\\data\\disease \n",
      "Successfully created the directory C:\\Users\\ivani\\source\\python\\msc_onlab1\\pubmed\\data\\chemistry \n"
     ]
    }
   ],
   "source": [
    "for category in categories:\n",
    "    dir_name = category.replace(' ', '_')\n",
    "    path_end = f\"\\\\{dir_name}\"\n",
    "\n",
    "    full_path = path_to_save + path_end\n",
    "    try:\n",
    "        os.mkdir(full_path)\n",
    "    except OSError:\n",
    "        print (\"Creation of the directory %s failed\" % full_path)\n",
    "    else:\n",
    "        print (\"Successfully created the directory %s \" % full_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select unique urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_groups = []\n",
    "for url_group in url_groups:\n",
    "    unique_pages = list({r['link']:r for r in url_group}.values())\n",
    "    unique_groups.append(unique_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and save pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for group, category in zip(unique_groups, categories):\n",
    "    dir_name = category.replace(' ', '_')\n",
    "    for page, i in zip(group, range(1, len(group)+1)):\n",
    "        response = requests.get(base_url_path+page['link'], headers=headers, verify=False)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        with open(f\"{path_to_save}\\\\{dir_name}\\\\{category}_{i}.html\", \"w\", encoding='utf-8') as file:\n",
    "            file.write(str(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
