{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from sklearn import model_selection\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './api_data'\n",
    "ignore_files = []\n",
    "data_files = os.listdir(data_dir)\n",
    "files = list(set(data_files)-set(ignore_files))\n",
    "\n",
    "categories = files\n",
    "\n",
    "predict_from = {'abstract': True, 'title': False, 'full_page': False}\n",
    "\n",
    "files_codes = dict(zip(files, range(0, len(files))))\n",
    "do_preproc = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(route, label):\n",
    "    pages_df = pd.DataFrame(columns=['document', 'category'])\n",
    "    f = open(route, \"rt\", errors='ignore')\n",
    "    articles = f.read()\n",
    "    f.close()\n",
    "    soup_atricles = BeautifulSoup(articles, 'xml')\n",
    "    article_list = soup_atricles.find_all('PubmedArticle')\n",
    "    for article, i in zip(article_list, range(0, len(article_list))):\n",
    "        extracted = ''\n",
    "        if predict_from['full_page']:\n",
    "            extracted = article.get_text()\n",
    "        elif predict_from['title']:\n",
    "            extracted = article.find('ArticleTitle').get_text()\n",
    "        elif predict_from['abstract']:\n",
    "            maybe_abstract = article.find(\"Abstract\")\n",
    "            if maybe_abstract:\n",
    "                extracted = maybe_abstract.get_text()\n",
    "        \n",
    "        extracted = re.sub(\"\\W\",\" \",extracted)\n",
    "        extracted = re.sub(r\" +\",\" \",extracted)\n",
    "        \n",
    "        pages_df.loc[i] = (extracted, label)\n",
    "    return pages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cysticfibrosis -ready\n",
      "pneumonia -ready\n",
      "COPD -ready\n",
      "asthma -ready\n",
      "acutebronchitis -ready\n",
      "flu -ready\n",
      "lungcancer -ready\n"
     ]
    }
   ],
   "source": [
    "all_pages = pd.DataFrame(columns=['document', 'category', 'label'])\n",
    "\n",
    "train_x = pd.Series()\n",
    "valid_x = pd.Series()\n",
    "train_y = pd.Series()\n",
    "valid_y = pd.Series()\n",
    "\n",
    "for file_name in files_codes:\n",
    "    category = file_name.split('_')[0]\n",
    "    code = files_codes[file_name]\n",
    "    \n",
    "    #Drop empty rows\n",
    "    pages_df = create_df(data_dir+f'/{file_name}', category)\n",
    "    pages_df.replace('', np.nan, inplace=True)\n",
    "    pages_df.dropna(subset=['document'], inplace=True)\n",
    "\n",
    "    pages_df['label'] =code\n",
    "    \n",
    "    all_pages = all_pages.append(pages_df, ignore_index=True)\n",
    "    \n",
    "    #Representative train-val sets\n",
    "    train_x_, valid_x_, train_y_, valid_y_ = model_selection.train_test_split(pages_df['document'], pages_df['label'], test_size =  0.15)\n",
    "    \n",
    "    #Append to main dataframe\n",
    "    train_x = train_x.append(train_x_, ignore_index=True)\n",
    "    valid_x = valid_x.append(valid_x_, ignore_index=True)\n",
    "    train_y = train_y.append(train_y_, ignore_index=True)\n",
    "    valid_y = valid_y.append(valid_y_, ignore_index=True)\n",
    "    print(category, \"-ready\")\n",
    "train_x, train_y = shuffle(train_x, train_y, random_state=42)\n",
    "valid_x, valid_y = shuffle(valid_x, valid_y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pages.document = all_pages.apply(lambda x: x.document.lower(), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pages['doc_list'] = all_pages.apply(lambda x: x.document.split(), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    6207.000000\n",
       "mean      245.030127\n",
       "std        95.749661\n",
       "min         0.000000\n",
       "25%       190.000000\n",
       "50%       249.000000\n",
       "75%       290.000000\n",
       "max      1079.000000\n",
       "Name: doc_list_len, dtype: float64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pages['doc_list_len'] = all_pages.apply(lambda x: len(x.doc_list), axis = 1)\n",
    "all_pages['doc_list_len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array(all_pages.label.astype(int))\n",
    "deep_target = np.zeros((a.size, a.max()+1))\n",
    "deep_target[np.arange(a.size),a] = 1\n",
    "deep_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(all_pages['doc_list'], size=200, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import text, sequence\n",
    "from tensorflow.keras import layers, models, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.zeros(shape=(all_pages.shape[0], 200) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "# our corpus\n",
    "data = all_pages['document']\n",
    "\n",
    "cv = CountVectorizer()\n",
    "\n",
    "# convert text data into term-frequency matrix\n",
    "data = cv.fit_transform(data)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "# convert term-frequency matrix into tf-idf\n",
    "tfidf_matrix = tfidf_transformer.fit_transform(data)\n",
    "\n",
    "# create dictionary to find a tfidf word each word\n",
    "word2tfidf = dict(zip(cv.get_feature_names(), tfidf_transformer.idf_))\n",
    "\n",
    "dictis = {}\n",
    "for word, score in word2tfidf.items():\n",
    "    dictis[word] = score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc to vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, row in all_pages.iterrows():\n",
    "    sentence_vec = np.zeros(shape = (200,))\n",
    "    for word in row['doc_list']:\n",
    "        if word in dictis:\n",
    "            word_vec = model[word] * dictis[word]\n",
    "        else:\n",
    "            word_vec = model[word]\n",
    "        sentence_vec = np.add(word_vec, sentence_vec)\n",
    "    sentence_vec = sentence_vec / len(row['doc_list'])\n",
    "    dataset[ind] = sentence_vec *10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shallow neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = dataset[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.keras.utils.normalize(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "nans = np.isnan(dataset)\n",
    "dataset[nans] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(dataset, deep_target, test_size =  0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create input layer \n",
    "input_layer = layers.Input(input_size, sparse=True)\n",
    "\n",
    "# create hidden layer\n",
    "hidden_layer = layers.Dense(200, activation=\"tanh\")(input_layer)\n",
    "\n",
    "# create output layer\n",
    "output_layer = layers.Dense(7, activation=\"softmax\")(hidden_layer)\n",
    "\n",
    "classifier = models.Model(inputs = input_layer, outputs = output_layer)\n",
    "classifier.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_21 (InputLayer)        [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 7)                 1407      \n",
      "=================================================================\n",
      "Total params: 41,607\n",
      "Trainable params: 41,607\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "176/176 [==============================] - 0s 1ms/step - loss: 0.8806 - accuracy: 0.7098\n",
      "Epoch 2/30\n",
      "176/176 [==============================] - 0s 970us/step - loss: 0.8829 - accuracy: 0.7094\n",
      "Epoch 3/30\n",
      "176/176 [==============================] - 0s 991us/step - loss: 0.8802 - accuracy: 0.7050\n",
      "Epoch 4/30\n",
      "176/176 [==============================] - 0s 989us/step - loss: 0.8753 - accuracy: 0.7113\n",
      "Epoch 5/30\n",
      "176/176 [==============================] - 0s 987us/step - loss: 0.8750 - accuracy: 0.7100\n",
      "Epoch 6/30\n",
      "176/176 [==============================] - 0s 1ms/step - loss: 0.8798 - accuracy: 0.7096\n",
      "Epoch 7/30\n",
      "176/176 [==============================] - 0s 1ms/step - loss: 0.8744 - accuracy: 0.7069\n",
      "Epoch 8/30\n",
      "176/176 [==============================] - 0s 1ms/step - loss: 0.8695 - accuracy: 0.7098\n",
      "Epoch 9/30\n",
      "176/176 [==============================] - 0s 1ms/step - loss: 0.8746 - accuracy: 0.7103\n",
      "Epoch 10/30\n",
      "176/176 [==============================] - 0s 1ms/step - loss: 0.8765 - accuracy: 0.7105\n",
      "Epoch 11/30\n",
      "176/176 [==============================] - 0s 1ms/step - loss: 0.8709 - accuracy: 0.7107\n",
      "Epoch 12/30\n",
      "176/176 [==============================] - 0s 1ms/step - loss: 0.8703 - accuracy: 0.7143\n",
      "Epoch 13/30\n",
      "176/176 [==============================] - 0s 960us/step - loss: 0.8684 - accuracy: 0.7136\n",
      "Epoch 14/30\n",
      "176/176 [==============================] - 0s 967us/step - loss: 0.8629 - accuracy: 0.7160\n",
      "Epoch 15/30\n",
      "176/176 [==============================] - 0s 1ms/step - loss: 0.8757 - accuracy: 0.7092\n",
      "Epoch 16/30\n",
      "176/176 [==============================] - 0s 1ms/step - loss: 0.8696 - accuracy: 0.7139\n",
      "Epoch 17/30\n",
      "176/176 [==============================] - 0s 1ms/step - loss: 0.8685 - accuracy: 0.7062\n",
      "Epoch 18/30\n",
      "176/176 [==============================] - 0s 965us/step - loss: 0.8665 - accuracy: 0.7136\n",
      "Epoch 19/30\n",
      "176/176 [==============================] - 0s 989us/step - loss: 0.8623 - accuracy: 0.7156\n",
      "Epoch 20/30\n",
      "176/176 [==============================] - 0s 985us/step - loss: 0.8625 - accuracy: 0.7130\n",
      "Epoch 21/30\n",
      "176/176 [==============================] - 0s 950us/step - loss: 0.8642 - accuracy: 0.7141\n",
      "Epoch 22/30\n",
      "176/176 [==============================] - 0s 969us/step - loss: 0.8628 - accuracy: 0.7170\n",
      "Epoch 23/30\n",
      "176/176 [==============================] - 0s 1ms/step - loss: 0.8593 - accuracy: 0.7124\n",
      "Epoch 24/30\n",
      "176/176 [==============================] - 0s 1ms/step - loss: 0.8597 - accuracy: 0.7141\n",
      "Epoch 25/30\n",
      "176/176 [==============================] - 0s 1ms/step - loss: 0.8626 - accuracy: 0.7120\n",
      "Epoch 26/30\n",
      "176/176 [==============================] - 0s 1ms/step - loss: 0.8621 - accuracy: 0.7170\n",
      "Epoch 27/30\n",
      "176/176 [==============================] - 0s 1ms/step - loss: 0.8619 - accuracy: 0.7141\n",
      "Epoch 28/30\n",
      "176/176 [==============================] - 0s 995us/step - loss: 0.8597 - accuracy: 0.7139\n",
      "Epoch 29/30\n",
      "176/176 [==============================] - 0s 1ms/step - loss: 0.8609 - accuracy: 0.7162\n",
      "Epoch 30/30\n",
      "176/176 [==============================] - 0s 967us/step - loss: 0.8592 - accuracy: 0.7126\n",
      "30/30 [==============================] - 0s 813us/step - loss: 0.8585 - accuracy: 0.7071\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8585413098335266, 0.7070815563201904]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(x=train_x, y=train_y, batch_size=30, epochs=30)\n",
    "classifier.evaluate(x=valid_x, y=valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.zeros(shape=(all_pages.shape[0], 300, 200) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ivani\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"\n",
      "C:\\Users\\ivani\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "for ind, row in all_pages.iterrows():\n",
    "    sentence_vec = np.zeros(shape = (300, 200))\n",
    "    for i, word in enumerate(row['doc_list'][0:300]):\n",
    "        if word in dictis:\n",
    "            word_vec = model[word] * dictis[word]\n",
    "        else:\n",
    "            word_vec = model[word]\n",
    "        sentence_vec[i] = word_vec * 10\n",
    "    dataset[ind] = sentence_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = dataset[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-140-dce9b661b32c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\np_utils.py\u001b[0m in \u001b[0;36mnormalize\u001b[1;34m(x, axis, order)\u001b[0m\n\u001b[0;32m     74\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mnormalized\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m   \"\"\"\n\u001b[1;32m---> 76\u001b[1;33m   \u001b[0ml2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m   \u001b[0ml2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mnorm\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\numpy\\linalg\\linalg.py\u001b[0m in \u001b[0;36mnorm\u001b[1;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[0;32m   2504\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mord\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mord\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2505\u001b[0m             \u001b[1;31m# special case for speedup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2506\u001b[1;33m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2507\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2508\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = tf.keras.utils.normalize(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nans = np.isnan(dataset)\n",
    "dataset[nans] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(dataset, deep_target, test_size =  0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = layers.Input(input_size)\n",
    "lstm_layer = layers.LSTM(200)(input_layer)\n",
    "output_layer1 = layers.Dense(100, activation=\"tanh\")(lstm_layer)\n",
    "output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "output_layer2 = layers.Dense(7, activation=\"softmax\")(output_layer1)\n",
    "\n",
    "classifier = models.Model(inputs = input_layer, outputs = output_layer2)\n",
    "classifier.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier.fit(x=train_x, y=train_y, batch_size=30, epochs=30)\n",
    "classifier.evaluate(x=valid_x, y=valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
